import requests
from bs4 import BeautifulSoup
import openpyxl
import json
import re
import time
import argparse
import sys
import os
from datetime import datetime
# --- –ü–£–¢–¨ –ö –¢–ï–ö–£–©–ï–ú–£ –°–ö–†–ò–ü–¢–£ ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# --- –ü–ê–ü–ö–ê –î–õ–Ø –§–ê–ô–õ–û–í ---
EXCEL_DIR = os.path.join(BASE_DIR, "excel_category")

# —Å–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs(EXCEL_DIR, exist_ok=True)

BASE_URL = "https://lalafo.kg"
START_URL = "https://lalafo.kg/kyrgyzstan"
HEADERS = { "User-Agent": "Mozilla/5.0" }
sys.stdout.reconfigure(encoding='utf-8')
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è URL –ø–æ –Ω–æ–º–µ—Ä—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
def get_category_urls(category_number):
    full_url = categories.get(category_number)
    if full_url:
        short_url = "/" + full_url.split("/")[-1]
        return full_url, short_url
    else:
        return None, None
        
def remove_duplicates_excel(input_file, url_column=1):
    """
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏ –≤ Excel –ø–æ URL (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø–µ—Ä–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü) –∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª.

    :param input_file: –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É Excel —Ñ–∞–π–ª—É
    :param url_column: –Ω–æ–º–µ—Ä —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (1 = A)
    """
    wb = openpyxl.load_workbook(input_file)
    sheet = wb.active

    seen = set()
    rows_to_keep = []

    # –ü—Ä–æ–±–µ–≥–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ (–∫—Ä–æ–º–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞)
    for i, row in enumerate(sheet.iter_rows(values_only=True)):
        if i == 0:
            rows_to_keep.append(row)  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            continue
        url = row[url_column - 1]  # —Å—Ç–æ–ª–±–µ—Ü URL
        if url not in seen:
            seen.add(url)
            rows_to_keep.append(row)

    # –û—á–∏—â–∞–µ–º –ª–∏—Å—Ç –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
    sheet.delete_rows(1, sheet.max_row)
    for row in rows_to_keep:
        sheet.append(row)

    wb.save(input_file)
    print(f"‚úÖ –î—É–±–ª–∏–∫–∞—Ç—ã —É–¥–∞–ª–µ–Ω—ã. –§–∞–π–ª –æ–±–Ω–æ–≤–ª—ë–Ω: {input_file}")


# ----------- –í–´–¢–ê–°–ö–ò–í–ê–ï–ú –í–°–ï –ö–ê–¢–ï–ì–û–†–ò–ò -----------
def get_all_categories():
    print("üìÅ –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
    r = requests.get(START_URL, headers=HEADERS)
    soup = BeautifulSoup(r.text, "html.parser")
    categories = set()
    for a in soup.select("a[href^='/kyrgyzstan/']"):
        href = a.get("href")
        if "/ads/" in href:
            continue
        full = BASE_URL + href
        categories.add(full)
    print(f"üìå –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(categories)}")
    return list(categories)

# ----------- –í–´–¢–ê–°–ö–ò–í–ê–ï–ú –°–°–´–õ–ö–ò –ù–ê –û–ë–™–Ø–í–õ–ï–ù–ò–Ø -----------
def parse_ads_from_page(url):
    r = requests.get(url, headers=HEADERS)
    if r.status_code != 200:
        return []
    soup = BeautifulSoup(r.text, "html.parser")
    ads = set()
    for a in soup.select("a[href*='/ads/']"):
        href = a.get("href")
        if href.startswith("/"):
            ads.add(BASE_URL + href)
    return list(ads)

def find_text_detailed(url):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π
    """
    try:
        #print(f"üîç –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {url}")
        
        # –ü–æ–ª—É—á–∞–µ–º HTML
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
        
        #print(f"üìä –°—Ç–∞—Ç—É—Å –∫–æ–¥: {response.status_code}")
        #print(f"üìè –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(response.text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
        #print(f"üî§ –ö–æ–¥–∏—Ä–æ–≤–∫–∞: {response.encoding}")
        
        # –ü–∞—Ä—Å–∏–º HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        for script in soup(["script", "style"]):
            script.decompose()
        
        # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
        page_text = soup.get_text()
        page_text = ' '.join(page_text.split())  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        
        #print(f"üìù –¢–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):")
        #print(page_text[:500])
        #print("-" * 50)
        
        # –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ –ø–æ–∏—Å–∫–∞
        search_phrases = [
            '–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å',
            '–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å'.lower(),
            '–≤–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å',  # –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        ]
        
        found = False
        for phrase in search_phrases:
            if phrase in page_text:
                #print(f'‚úÖ –¢–µ–∫—Å—Ç "{phrase}" –Ω–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ!')
                found = True
                # –ù–∞–π–¥–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                index = page_text.find(phrase)
                context = page_text[max(0, index-50):index+100]
                print(f"üìã –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context}...")
                break
            elif phrase in response.text:
                #print(f'‚ö†Ô∏è –¢–µ–∫—Å—Ç "{phrase}" –Ω–∞–π–¥–µ–Ω –≤ HTML, –Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ –≤ —Ç–µ–≥–∞—Ö/–∞—Ç—Ä–∏–±—É—Ç–∞—Ö')
                found = True
                break
        
        if not found:
            #print('‚ùå –¢–µ–∫—Å—Ç "–í–∞—Å –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å" –Ω–µ –Ω–∞–π–¥–µ–Ω')
            
            # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–∑
            similar_patterns = [
                r'–≤–∞—Å[\s]+–º–æ–∂–µ—Ç[\s]+–∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å',
                r'–í–∞—Å[\s]+–º–æ–∂–µ—Ç[\s]+–∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å',
                r'–∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å',
            ]
            
            for pattern in similar_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                if matches:
                    print(f'üîç –ù–∞–π–¥–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–∑—ã: {matches}')
        
        return found
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        return False

    
# ----------- –í–´–¢–ê–°–ö–ò–í–ê–ï–ú –î–ê–ù–ù–´–ï –û–ë–™–Ø–í–õ–ï–ù–ò–Ø -----------
def extract_user_data(url):
    try:
        r = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(r.text, "html.parser")
        script_tag = soup.find("script", id="__NEXT_DATA__")
        if not script_tag:
            return None, None, None, None, None, None
        data = json.loads(script_tag.string)
        phone = find_deep(data, ["mobile", "phone", "telephone", "contact"])
        username = find_deep(data, ["username", "user_name"])
        city = find_deep(data, ["city", "City"])
        createdTime = find_deep(data, ["created_time"],allow_numbers=True)
        updatedTime = find_deep(data, ["updated_time"],allow_numbers=True)
        createdTimeStr = timestamp_to_date(createdTime)
        updatedTimeStr = timestamp_to_date(updatedTime)
        title = find_deep(data, ["title"])
        #print(f'üîç createdTime =================================== {createdTimeStr}')
        if phone:
            phone = re.sub(r"[^\d+]", "", phone)
        return phone, username, city , createdTimeStr, updatedTimeStr, title
    except:
        return None, None, None, None, None, None


# ----------- –†–ï–ö–£–†–°–ò–í–ù–´–ô –ü–û–ò–°–ö –í JSON -----------
def find_deep(obj, keys, allow_numbers=False):
    if isinstance(obj, dict):
        for k in keys:
            if k in obj:
                value = obj[k]
                # –†–∞–∑—Ä–µ—à–∞–µ–º —á–∏—Å–ª–∞ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∏
                if (isinstance(value, str) and value.strip()) or (allow_numbers and isinstance(value, (int, float))):
                    return value
        for v in obj.values():
            result = find_deep(v, keys, allow_numbers)
            if result:
                return result
    elif isinstance(obj, list):
        for item in obj:
            result = find_deep(item, keys, allow_numbers)
            if result:
                return result
    return None

def timestamp_to_date(timestamp):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Unix timestamp –≤ —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ DD.MM.YY"""
    if timestamp:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –∏–ª–∏ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö timestamp
            if timestamp > 10000000000:  # –ï—Å–ª–∏ —á–∏—Å–ª–æ –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–µ - —ç—Ç–æ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã
                timestamp = timestamp / 1000
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º timestamp –≤ datetime –æ–±—ä–µ–∫—Ç
            dt = datetime.fromtimestamp(timestamp)
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            return dt.strftime("%d.%m.%y")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ timestamp {timestamp}: {e}")
            return None
    return None
    
# ----------- –°–û–ó–î–ê–Å–ú EXCEL -----------
wb = openpyxl.Workbook()
sheet = wb.active
sheet.append(["URL", "–ù–∞–∑–≤–∞–Ω–∏–µ", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–¢–µ–ª–µ—Ñ–æ–Ω", "–õ–æ–∫–∞—Ü–∏—è","–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è","–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è" "–ö–∞—Ç–µ–≥–æ—Ä–∏—è"])

# ----------- –ó–ê–ü–£–°–ö –ü–ê–†–°–ò–ù–ì–ê –° –õ–û–ì–ò–ö–û–ô –°–ß–Å–¢–ß–ò–ö–ê -----------
#categories = get_all_categories()

category = ""    
if len(sys.argv) > 1:
    category = sys.argv[1]
    print("–ü–µ—Ä–≤—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç:", category)
    

cat_url = category
if cat_url:
    short_url = "/" + cat_url.split("/")[-1]
    print("–ü–æ–ª–Ω—ã–π URL:", cat_url)
    print("–ö–æ—Ä–æ—Ç–∫–∏–π URL:", short_url)
else:
    print("–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å —Ç–∞–∫–∏–º –Ω–æ–º–µ—Ä–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    
page = 1
empty_pages = 0

# –£–±–∏—Ä–∞–µ–º —Å–ª—ç—à –≤ –Ω–∞—á–∞–ª–µ, —á—Ç–æ–±—ã –∏–º—è —Ñ–∞–π–ª–∞ –±—ã–ª–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º
short_url_clean = short_url.lstrip("/")

# –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
filename = f"lalafo_ads_{short_url_clean}.xlsx"
exelFileName = f"lalafo_ads_{short_url_clean}.xlsx"
while True:
    page_url = f"{cat_url}?page={page}"
    print(f"  üîç –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}")
    isFinishTextFound = find_text_detailed(page_url)
    ads = parse_ads_from_page(page_url)
    if not ads:
        empty_pages += 1
        print(f"   üö´ –ü—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}")
        if empty_pages >= 2:  # 2 –ø—É—Å—Ç—ã–µ –ø–æ–¥—Ä—è–¥ ‚Üí —Å—Ç–æ–ø
            break
    else:
        empty_pages = 0
        print(f"   ‚û° –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(ads)}")
        for ad in ads:
            print(f"     ‚Üí –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {ad}")
            phone, user, city, createdTime, updatedTime, title = extract_user_data(ad)
            sheet.append([ad, title, user, phone, city, createdTime, updatedTime, cat_url])
            time.sleep(0.3)

    exelFileName = os.path.join(EXCEL_DIR, filename)
    wb.save(exelFileName)
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ø–æ—Å–ª–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
    if page % 5 == 0:
        remove_duplicates_excel(exelFileName)
    page += 1
    
    if isFinishTextFound:
        break;
        
remove_duplicates_excel(exelFileName)
print(f"üéâ –ì–æ—Ç–æ–≤–æ! –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å–ø–∞—Ä—Å–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {short_url_clean}")
